{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0122 14:43:40.476399 140736180405120 file_utils.py:35] PyTorch version 1.3.1 available.\n",
      "I0122 14:43:43.374063 140736180405120 file_utils.py:48] TensorFlow version 2.0.0 available.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "# Encode a text inputs\n",
    "# text = \"Who was Jim Henson ? Jim Henson was a\"\n",
    "# indexed_tokens = tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0122 14:43:50.259464 140736180405120 file_utils.py:362] https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json not found in cache or force_download set to True, downloading to /var/folders/qx/0nm1448x2jd_ghmvg1wd3xwr0000gn/T/tmpntbxhy3a\n",
      "I0122 14:43:56.653349 140736180405120 file_utils.py:377] copying /var/folders/qx/0nm1448x2jd_ghmvg1wd3xwr0000gn/T/tmpntbxhy3a to cache at /Users/admin/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "I0122 14:43:56.657879 140736180405120 file_utils.py:381] creating metadata file for /Users/admin/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "I0122 14:43:56.659898 140736180405120 file_utils.py:390] removing temp file /var/folders/qx/0nm1448x2jd_ghmvg1wd3xwr0000gn/T/tmpntbxhy3a\n",
      "I0122 14:43:58.122875 140736180405120 file_utils.py:362] https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt not found in cache or force_download set to True, downloading to /var/folders/qx/0nm1448x2jd_ghmvg1wd3xwr0000gn/T/tmprlws29d8\n",
      "I0122 14:44:01.468748 140736180405120 file_utils.py:377] copying /var/folders/qx/0nm1448x2jd_ghmvg1wd3xwr0000gn/T/tmprlws29d8 to cache at /Users/admin/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0122 14:44:01.471740 140736180405120 file_utils.py:381] creating metadata file for /Users/admin/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
      "I0122 14:44:01.473318 140736180405120 file_utils.py:390] removing temp file /var/folders/qx/0nm1448x2jd_ghmvg1wd3xwr0000gn/T/tmprlws29d8\n",
      "I0122 14:44:01.477421 140736180405120 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /Users/admin/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "I0122 14:44:01.478605 140736180405120 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /Users/admin/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10919, 262, 5089]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('what the fuck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method GPT2Tokenizer.bpe of <transformers.tokenization_gpt2.GPT2Tokenizer object at 0x111075940>>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bpe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
